# K-최근접 이웃 (K-Nearest Neighbours)

이 문서는 `k_nearest_neighbours.py` 파일에 구현된 **K-최근접 이웃 (KNN)** 분류 알고리즘에 대해 설명합니다.

## 개요

K-최근접 이웃 알고리즘은 지도 학습(Supervised Learning)의 일종으로, 새로운 데이터 포인트가 주어졌을 때 가장 가까운 $k$개의 이웃 데이터를 찾아 그 중 가장 빈도수가 높은 클래스로 분류하는 방법입니다. 별도의 학습 과정이 필요 없는 게으른 학습(Lazy Learning) 모델입니다.

이 코드는 `scikit-learn`의 붓꽃(Iris) 데이터셋을 사용하여 KNN 알고리즘을 직접 구현하고 테스트합니다.

## 주요 함수

### `euclidean_distance(a, b)`

- **목적**: 두 점 `a`와 `b` 사이의 유클리드 거리(Euclidean Distance)를 계산합니다.
- **수식**: $\sqrt{\sum (a_i - b_i)^2}$
- **매개변수**:
  - `a`, `b`: 좌표를 나타내는 리스트 또는 배열.
- **반환값**: 두 점 사이의 거리 (실수).

### `classifier(train_data, train_target, classes, point, k=5)`

- **목적**: 주어진 데이터 포인트 `point`를 KNN 알고리즘을 사용하여 분류합니다.
- **매개변수**:
  - `train_data`: 학습 데이터셋 (특징 벡터들의 리스트).
  - `train_target`: 학습 데이터의 레이블(클래스 인덱스) 리스트.
  - `classes`: 클래스 이름들의 리스트 (예: `['setosa', 'versicolor', 'virginica']`).
  - `point`: 분류할 새로운 데이터 포인트.
  - `k`: 고려할 이웃의 수 (기본값: 5).
- **알고리즘 동작**:
  1. `train_data`의 모든 점과 입력 `point` 사이의 유클리드 거리를 계산합니다.
  2. 거리가 가까운 순서대로 정렬하여 상위 `k`개의 이웃을 선택합니다.
  3. 선택된 `k`개의 이웃들이 가진 클래스 중 가장 많이 등장한 클래스(다수결)를 찾습니다.
  4. 해당 클래스의 이름을 반환합니다.

## 사용법

`if __name__ == "__main__":` 블록에서 실행 예시를 확인할 수 있습니다.

1. `sklearn.datasets`에서 Iris 데이터를 로드합니다.
2. 데이터를 학습용과 테스트용으로 분리합니다 (이 코드에서는 분리만 하고 `classifier` 함수 테스트에는 직접 값을 입력하여 사용합니다).
3. 임의의 데이터 포인트 `[4.4, 3.1, 1.3, 1.4]`에 대해 분류를 수행하고 결과를 출력합니다.

```python
# 실행 예시
print(classifier(X_train, y_train, classes, [4.4, 3.1, 1.3, 1.4]))
# 출력: versicolor (또는 데이터 분할에 따라 달라질 수 있음)
```

## 요구 사항
- `numpy`: 벡터 연산 및 거리 계산.
- `sklearn`: 데이터셋 로드 및 분할.
